{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:13:36.887497Z",
     "start_time": "2025-01-24T20:13:35.899059Z"
    }
   },
   "cell_type": "code",
   "source": "import org.apache.spark",
   "id": "704415aa2ad0419e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "val sc = spark.SparkContext.getOrCreate()",
   "id": "adcfb3bdf442709a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:13:40.438900Z",
     "start_time": "2025-01-24T20:13:39.315463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_datasets = \"../../../datasets/processed\"\n",
    "\n",
    "val path_to_tracks = path_to_datasets + \"/tracks.csv\"\n",
    "val path_to_playlists = path_to_datasets + \"/playlists.csv\"\n",
    "val path_to_track_in_playlists = path_to_datasets + \"/tracks_in_playlist.csv\"\n",
    "val path_to_artists = path_to_datasets + \"/artists.csv\""
   ],
   "id": "2b4186c7affd9f48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../datasets/processed\r\n",
       "path_to_tracks: String = ../../../datasets/processed/tracks.csv\r\n",
       "path_to_playlists: String = ../../../datasets/processed/playlists.csv\r\n",
       "path_to_track_in_playlists: String = ../../../datasets/processed/tracks_in_playlist.csv\r\n",
       "path_to_artists: String = ../../../datasets/processed/artists.csv\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:13:44.567103Z",
     "start_time": "2025-01-24T20:13:43.390612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object CsvParser {\n",
    "\n",
    "  val noGenresListed = \"(no genres listed)\"\n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val quotes = \"\\\"\"\n",
    "\n",
    "\n",
    "  // (PID, playlist_name, num_followers)\n",
    "  def parsePlayListLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (track_uri, track_name, duration_ms, artist_uri, album_uri, album_name)\n",
    "  def parseTrackLine(line: String): Option[(String, String, Int, String, String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt, input(3).trim, input(4).trim, input(5).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (artist_uri, artist_name)\n",
    "  def parseArtistLine(line: String): Option[(String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (PID, track_uri, pos)\n",
    "  def parseTrackInPlaylistLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "id": "2055b63ab14ea011",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object CsvParser\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:13:50.066448Z",
     "start_time": "2025-01-24T20:13:47.588517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddTracks = sc.textFile(path_to_tracks).\n",
    "  flatMap(CsvParser.parseTrackLine)\n",
    "\n",
    "val rddPlaylists = sc.textFile(path_to_playlists).\n",
    "  flatMap(CsvParser.parsePlayListLine)\n",
    "\n",
    "val rddTrackInPlaylists = sc.textFile(path_to_track_in_playlists).\n",
    "  flatMap(CsvParser.parseTrackInPlaylistLine)\n",
    "\n",
    "val rddArtists = sc.textFile(path_to_artists).\n",
    "  flatMap(CsvParser.parseArtistLine)"
   ],
   "id": "b6fd9a151f40dcab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTracks: org.apache.spark.rdd.RDD[(String, String, Int, String, String, String)] = MapPartitionsRDD[9] at flatMap at <console>:33\r\n",
       "rddPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[12] at flatMap at <console>:36\r\n",
       "rddTrackInPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[15] at flatMap at <console>:39\r\n",
       "rddArtists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[18] at flatMap at <console>:42\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:30:51.074226Z",
     "start_time": "2025-01-24T20:30:50.564445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// ID of the song to analyze\n",
    "val idSong = \"spotify:track:5xlWA3V2l7ZiqYF8Ag5EM8\""
   ],
   "id": "d1e28816c94572d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idSong: String = spotify:track:0UaMYEvWZi0ZqiDOoHU3YI\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:30:52.701594Z",
     "start_time": "2025-01-24T20:30:52.286880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// RDD of (pid, trackUri)\n",
    "val trackInPlaylistReduce = rddTrackInPlaylists.map(x => (x._1, x._2))"
   ],
   "id": "fcb79592d2ca2e7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackInPlaylistReduce: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[33] at map at <console>:29\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:30:54.932197Z",
     "start_time": "2025-01-24T20:30:54.261173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// filter to obtain the id of playlists that contains the track\n",
    "val playlistForTrack = trackInPlaylistReduce\n",
    "          .filter { case (_, trackUri) => trackUri == idSong }\n",
    "          .map(_._1)\n",
    "          .distinct()"
   ],
   "id": "efbb6022f43e225c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playlistForTrack: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[35] at map at <console>:31\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:31:11.901183Z",
     "start_time": "2025-01-24T20:31:01.480174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// Broadcast the list of playlists\n",
    "val playlistsBroadcast = sc.broadcast(playlistForTrack.collect().toSet)"
   ],
   "id": "ab3bec368ff21243",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playlistsBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(15)\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:31:35.108615Z",
     "start_time": "2025-01-24T20:31:34.701625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// filter to obtain all the songs in playlist that contains the track\n",
    "val trackInSamePlaylists = trackInPlaylistReduce\n",
    "          .filter { case (pid, _) => playlistsBroadcast.value.contains(pid) }\n",
    "          .filter { case (_, trackUri) => trackUri != idSong }\n",
    "\n",
    "// RDD of (pid, trackUri)"
   ],
   "id": "75a96a8049ffdd91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackInSamePlaylists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[37] at filter at <console>:32\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:31:48.441547Z",
     "start_time": "2025-01-24T20:31:47.955472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// create the pairs of ((mySong, otherSong), 1) for each playlist and reduce by key to count the occurrences\n",
    "val occurrencesCount = trackInSamePlaylists\n",
    "  .map { case (_, track) => ((idSong, track), 1) }\n",
    "  .reduceByKey(_ + _)\n"
   ],
   "id": "de3beb575405fec6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTrackPairs: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[38] at map at <console>:30\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:33:09.850206Z",
     "start_time": "2025-01-24T20:32:57.852966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// take the pair with the highest count\n",
    "val mostOccurrencesPair = occurrencesCount\n",
    "          .max()(Ordering.by(_._2))"
   ],
   "id": "305aa10a85ea1ed8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostOccurrencesPair: ((String, String), Int) = ((spotify:track:0UaMYEvWZi0ZqiDOoHU3YI,spotify:track:0XUfyU2QviPAs6bxSpXYG4),253)\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:34:12.606293Z",
     "start_time": "2025-01-24T20:34:00.465808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// mostOccurrencesPair to rdd to join \n",
    "val mostOccurrencesPairRDD = sc.parallelize(Seq(mostOccurrencesPair))\n",
    "\n",
    "// Unisci i dettagli della traccia specifica e delle tracce correlate\n",
    "val trackDetails = rddTracks.map(x => (x._1, x._2)) // (trackUri, trackName)\n",
    "\n",
    "val partitionedTrackDetails = trackDetails.partitionBy(new org.apache.spark.HashPartitioner(trackDetails.getNumPartitions))\n",
    "\n",
    "val enrichedResults = mostOccurrencesPairRDD\n",
    "        .map {case ((track1, track2), count) => (track1, (track2, count)) }\n",
    "        .join(partitionedTrackDetails) // Unisci il nome della traccia principale\n",
    "        .map { case (track1, ((track2, count), track1Name)) => (track2, (track1, track1Name, count)) }\n",
    "        .join(partitionedTrackDetails) // Unisci il nome delle co-tracce\n",
    "        .map { case (track2, ((track1, track1Name, count), track2Name)) => (track1, track1Name, track2, track2Name, count) }\n",
    "\n",
    "// Salva il risultato\n",
    "enrichedResults.coalesce(1).saveAsTextFile(\"output/result\")"
   ],
   "id": "a12a82b255d0622b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackDetails: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[40] at map at <console>:31\r\n",
       "mostOccurrencesPairRDD: org.apache.spark.rdd.RDD[((String, String), Int)] = ParallelCollectionRDD[41] at parallelize at <console>:34\r\n",
       "enrichedResults: org.apache.spark.rdd.RDD[(String, String, String, String, Int)] = MapPartitionsRDD[50] at map at <console>:41\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
