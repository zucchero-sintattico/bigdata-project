{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:12.299815Z",
     "start_time": "2025-01-22T16:44:52.251650Z"
    }
   },
   "cell_type": "code",
   "source": "import org.apache.spark",
   "id": "704415aa2ad0419e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-PSTRJPQO:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1737564300181)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "val sc = spark.SparkContext.getOrCreate()",
   "id": "adcfb3bdf442709a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:15.991778Z",
     "start_time": "2025-01-22T16:45:14.879169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_datasets = \"../../../datasets/processed\"\n",
    "\n",
    "val path_to_tracks = path_to_datasets + \"/tracks.csv\"\n",
    "val path_to_playlists = path_to_datasets + \"/playlists.csv\"\n",
    "val path_to_track_in_playlists = path_to_datasets + \"/tracks_in_playlist.csv\"\n",
    "val path_to_artists = path_to_datasets + \"/artists.csv\""
   ],
   "id": "2b4186c7affd9f48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../datasets/processed\r\n",
       "path_to_tracks: String = ../../../datasets/processed/tracks.csv\r\n",
       "path_to_playlists: String = ../../../datasets/processed/playlists.csv\r\n",
       "path_to_track_in_playlists: String = ../../../datasets/processed/tracks_in_playlist.csv\r\n",
       "path_to_artists: String = ../../../datasets/processed/artists.csv\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:19.429998Z",
     "start_time": "2025-01-22T16:45:17.794826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object CsvParser {\n",
    "\n",
    "  val noGenresListed = \"(no genres listed)\"\n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val quotes = \"\\\"\"\n",
    "\n",
    "\n",
    "  // (PID, playlist_name, num_followers)\n",
    "  def parsePlayListLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (track_uri, track_name, duration_ms, artist_uri, album_uri, album_name)\n",
    "  def parseTrackLine(line: String): Option[(String, String, Int, String, String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt, input(3).trim, input(4).trim, input(5).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (artist_uri, artist_name)\n",
    "  def parseArtistLine(line: String): Option[(String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (PID, track_uri, pos)\n",
    "  def parseTrackInPlaylistLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "id": "2055b63ab14ea011",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object CsvParser\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:25.374797Z",
     "start_time": "2025-01-22T16:45:22.211263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddTracks = sc.textFile(path_to_tracks).\n",
    "  flatMap(CsvParser.parseTrackLine)\n",
    "\n",
    "val rddPlaylists = sc.textFile(path_to_playlists).\n",
    "  flatMap(CsvParser.parsePlayListLine)\n",
    "\n",
    "val rddTrackInPlaylists = sc.textFile(path_to_track_in_playlists).\n",
    "  flatMap(CsvParser.parseTrackInPlaylistLine)\n",
    "\n",
    "val rddArtists = sc.textFile(path_to_artists).\n",
    "  flatMap(CsvParser.parseArtistLine)"
   ],
   "id": "b6fd9a151f40dcab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTracks: org.apache.spark.rdd.RDD[(String, String, Int, String, String, String)] = MapPartitionsRDD[2] at flatMap at <console>:31\r\n",
       "rddPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[5] at flatMap at <console>:34\r\n",
       "rddTrackInPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[8] at flatMap at <console>:37\r\n",
       "rddArtists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[11] at flatMap at <console>:40\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:28.223153Z",
     "start_time": "2025-01-22T16:45:26.926774Z"
    }
   },
   "cell_type": "code",
   "source": "val idTrack = \"spotify:track:1UOuZs8BPGMM6Ls0jP6BjQ\"",
   "id": "d1e28816c94572d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idTrack: String = spotify:track:1UOuZs8BPGMM6Ls0jP6BjQ\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:30.423096Z",
     "start_time": "2025-01-22T16:45:29.343637Z"
    }
   },
   "cell_type": "code",
   "source": "val playlistToTracks = rddTrackInPlaylists.map(x => (x._1, x._2))",
   "id": "319e39424e8e314d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playlistToTracks: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at map at <console>:25\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:45:53.632140Z",
     "start_time": "2025-01-22T16:45:31.664399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// filter the playlist that contains the track\n",
    "val playlistForTrack = playlistToTracks.filter { case (_,trackUri) =>\n",
    "  trackUri == idTrack\n",
    "}.map(_._1)\n",
    "\n",
    "val playlistsBroadcast = sc.broadcast(playlistForTrack.collect())\n",
    "val tracksInSamePlaylists = playlistToTracks.filter { case (pid, _) => playlistsBroadcast.value.contains(pid) }\n"
   ],
   "id": "362cbe080edfd938",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playlistForTrack: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at map at <console>:30\r\n",
       "playlistsBroadcast: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(5)\r\n",
       "tracksInSamePlaylists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[15] at filter at <console>:33\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:46:20.717297Z",
     "start_time": "2025-01-22T16:46:18.675954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val coTracksByPlaylist = tracksInSamePlaylists\n",
    "        .groupByKey()\n",
    "        .flatMap { case (_, tracks) =>\n",
    "          val trackList = tracks.toList\n",
    "          for {\n",
    "            coTrack <- trackList if coTrack != idTrack\n",
    "          } yield (idTrack, coTrack)\n",
    "        }\n"
   ],
   "id": "678f06ba043648fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coTracksByPlaylist: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[17] at flatMap at <console>:28\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:46:24.727889Z",
     "start_time": "2025-01-22T16:46:23.606782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val occurrenceCount = coTracksByPlaylist\n",
    "        .map { case (track, coTrack) => (track, Map(coTrack -> 1)) }\n",
    "        .aggregateByKey(Map[String, Int]())(\n",
    "          (acc, value) => {\n",
    "            value.foldLeft(acc) { case (map, (coTrack, count)) =>\n",
    "              map + (coTrack -> (map.getOrElse(coTrack, 0) + count))\n",
    "            }\n",
    "          },\n",
    "          (map1, map2) => {\n",
    "            map2.foldLeft(map1) { case (map, (coTrack, count)) =>\n",
    "              map + (coTrack -> (map.getOrElse(coTrack, 0) + count))\n",
    "            }\n",
    "          }\n",
    "        )"
   ],
   "id": "30a2e9adfd2814f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occurrenceCount: org.apache.spark.rdd.RDD[(String, scala.collection.immutable.Map[String,Int])] = ShuffledRDD[19] at aggregateByKey at <console>:27\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:46:28.634520Z",
     "start_time": "2025-01-22T16:46:27.796376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val mostCooccurringTrackPerTrack = occurrenceCount\n",
    "        .mapValues { occurrences =>\n",
    "          occurrences.maxBy(_._2)\n",
    "        }"
   ],
   "id": "ce36b1b67db1b820",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostCooccurringTrackPerTrack: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[20] at mapValues at <console>:26\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:47:30.703415Z",
     "start_time": "2025-01-22T16:46:51.037739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val trackDetails = rddTracks.map(line => (line._1, line._2))\n",
    "\n",
    "val enrichedResults = mostCooccurringTrackPerTrack\n",
    "        .join(trackDetails)\n",
    "        .map { case (trackUri, ((coTrackUri, count), trackName)) =>\n",
    "          (coTrackUri, (trackUri, trackName, count))\n",
    "        }\n",
    "        .join(trackDetails)\n",
    "        .map { case (coTrackUri, ((trackUri, trackName, count), coTrackName)) =>\n",
    "          (trackUri, trackName, coTrackUri, coTrackName, count)\n",
    "        }\n",
    "\n",
    "enrichedResults.coalesce(1).saveAsTextFile(\"output/result\")"
   ],
   "id": "799ad3d1314b25e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackDetails: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[31] at map at <console>:26\r\n",
       "enrichedResults: org.apache.spark.rdd.RDD[(String, String, String, String, Int)] = MapPartitionsRDD[39] at map at <console>:34\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:26:14.265640Z",
     "start_time": "2025-01-22T16:26:12.250993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/*\n",
    "//PER TUTTE LE CANZONI, DA FARE CON POCHI FILE\n",
    " \n",
    "val playlistToTracks = rddTrackInPlaylists\n",
    "  .map(x => (x._1, x._2))"
   ],
   "id": "a6dcfc0adb9f7aff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playlistToTracks: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at map at <console>:28\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:51:40.255689Z",
     "start_time": "2025-01-21T20:51:36.870786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val coTracksByPlaylist = playlistToTracks\n",
    "  .groupByKey() // Raggruppa le tracce per playlist\n",
    "  .flatMap { case (_, tracks) =>\n",
    "    val trackList = tracks.toList\n",
    "    for {\n",
    "      track <- trackList\n",
    "      coTrack <- trackList if track != coTrack\n",
    "    } yield (track, coTrack) // Genera coppie (track, coTrack)\n",
    "  }"
   ],
   "id": "20616bae24b98378",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coTracksByPlaylist: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[14] at flatMap at <console>:27\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:51:43.142782Z",
     "start_time": "2025-01-21T20:51:41.966858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val cooccurrenceCounts = coTracksByPlaylist\n",
    "  .map { case (track, coTrack) => (track, Map(coTrack -> 1)) } // Mappa tracce a un conteggio iniziale\n",
    "  .aggregateByKey(Map[String, Int]())(\n",
    "    (acc, value) => { // Combinatore locale\n",
    "      value.foldLeft(acc) { case (map, (coTrack, count)) =>\n",
    "        map + (coTrack -> (map.getOrElse(coTrack, 0) + count))\n",
    "      }\n",
    "    },\n",
    "    (map1, map2) => { // Combinatore globale\n",
    "      map2.foldLeft(map1) { case (map, (coTrack, count)) =>\n",
    "        map + (coTrack -> (map.getOrElse(coTrack, 0) + count))\n",
    "      }\n",
    "    }\n",
    "  )"
   ],
   "id": "f5692f3bbd9beef5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cooccurrenceCounts: org.apache.spark.rdd.RDD[(String, scala.collection.immutable.Map[String,Int])] = ShuffledRDD[16] at aggregateByKey at <console>:27\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:51:45.680141Z",
     "start_time": "2025-01-21T20:51:44.725164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val mostCooccurringTrackPerTrack = cooccurrenceCounts\n",
    "  .mapValues { cooccurrences =>\n",
    "    cooccurrences.maxBy(_._2) // Trova la traccia con il conteggio massimo\n",
    "  }"
   ],
   "id": "15138e6d5850fbe0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostCooccurringTrackPerTrack: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[17] at mapValues at <console>:26\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:54:08.102087Z",
     "start_time": "2025-01-21T20:52:08.454836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// Mappa i dettagli delle tracce\n",
    "val trackDetail = rddTracks.map(line => (line._1, line._2))\n",
    "\n",
    "// Aggiungi i dettagli\n",
    "val enrichedResults = mostCooccurringTrackPerTrack\n",
    "  .join(trackDetail) // Aggiungi il nome della traccia principale\n",
    "  .map { case (trackUri, ((coTrackUri, count), trackName)) =>\n",
    "    (coTrackUri, (trackUri, trackName, count))\n",
    "  }\n",
    "  .join(trackDetail) // Aggiungi il nome della traccia co-occurrente\n",
    "  .map { case (coTrackUri, ((trackUri, trackName, count), coTrackName)) =>\n",
    "    (trackUri, trackName, coTrackUri, coTrackName, count)\n",
    "  }\n",
    "\n",
    "enrichedResults.saveAsTextFile(\"output/result\") */\n"
   ],
   "id": "c28eabb3b9d53fb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackDetail: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[28] at map at <console>:27\r\n",
       "enrichedResults: org.apache.spark.rdd.RDD[(String, String, String, String, Int)] = MapPartitionsRDD[36] at map at <console>:36\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
