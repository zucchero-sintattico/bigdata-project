{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:42:36.452437Z",
     "start_time": "2025-01-27T14:42:31.895871Z"
    }
   },
   "cell_type": "code",
   "source": "import org.apache.spark",
   "id": "ed0887682c35b90f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.201.102.227:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1737988953691)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "val sc = spark.SparkContext.getOrCreate()",
   "id": "b23084ff7fb8f501"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:42:36.640340Z",
     "start_time": "2025-01-27T14:42:36.476462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_datasets = \"../../../datasets/processed\"\n",
    "\n",
    "val path_to_tracks = path_to_datasets + \"/tracks.csv\"\n",
    "val path_to_playlists = path_to_datasets + \"/playlists.csv\"\n",
    "val path_to_track_in_playlists = path_to_datasets + \"/tracks_in_playlist.csv\"\n",
    "val path_to_artists = path_to_datasets + \"/artists.csv\""
   ],
   "id": "dd2e2ec46e698b16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../datasets/processed\n",
       "path_to_tracks: String = ../../../datasets/processed/tracks.csv\n",
       "path_to_playlists: String = ../../../datasets/processed/playlists.csv\n",
       "path_to_track_in_playlists: String = ../../../datasets/processed/tracks_in_playlist.csv\n",
       "path_to_artists: String = ../../../datasets/processed/artists.csv\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:42:36.914683Z",
     "start_time": "2025-01-27T14:42:36.648972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object CsvParser {\n",
    "\n",
    "  val noGenresListed = \"(no genres listed)\"\n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val quotes = \"\\\"\"\n",
    "\n",
    "  // (PID, playlist_name, num_followers)\n",
    "  def parsePlayListLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (track_uri, track_name, duration_ms, artist_uri, album_uri, album_name)\n",
    "  def parseTrackLine(line: String): Option[(String, String, Int, String, String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt, input(3).trim, input(4).trim, input(5).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (artist_uri, artist_name)\n",
    "  def parseArtistLine(line: String): Option[(String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (PID, track_uri, pos)\n",
    "  def parseTrackInPlaylistLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "id": "5629fcf9175ed69f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object CsvParser\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:42:37.698877Z",
     "start_time": "2025-01-27T14:42:36.919719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddTracks = sc.textFile(path_to_tracks).\n",
    "  flatMap(CsvParser.parseTrackLine)\n",
    "\n",
    "val rddPlaylists = sc.textFile(path_to_playlists).\n",
    "  flatMap(CsvParser.parsePlayListLine)\n",
    "\n",
    "val rddTrackInPlaylists = sc.textFile(path_to_track_in_playlists).\n",
    "  flatMap(CsvParser.parseTrackInPlaylistLine)\n",
    "\n",
    "val rddArtists = sc.textFile(path_to_artists).\n",
    "  flatMap(CsvParser.parseArtistLine)"
   ],
   "id": "9800ecef096e915c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTracks: org.apache.spark.rdd.RDD[(String, (String, Int, String, String, String))] = MapPartitionsRDD[3] at map at <console>:32\n",
       "rddPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[6] at flatMap at <console>:38\n",
       "rddTrackInPlaylists: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[10] at map at <console>:42\n",
       "rddArtists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[13] at flatMap at <console>:47\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:42:37.993929Z",
     "start_time": "2025-01-27T14:42:37.705102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val numPartitions = sc.defaultParallelism\n",
    "// Numero di partizioni, dipende dalle risorse del cluster\n",
    "val partitioner = new HashPartitioner(numPartitions)\n",
    "\n",
    "val rddTrackInPlaylistsWithKey = rddTrackInPlaylists.keyBy(_._2)\n",
    "val rddTracksWithKey = rddTracks.keyBy(_._1)\n",
    "val rddArtistsWithKey = rddArtists.keyBy(_._1)\n",
    "\n",
    "val rddTracksInPlaylistTracks = rddTrackInPlaylistsWithKey\n",
    "  .join(rddTracksWithKey)\n",
    "  .map({\n",
    "    case (_, ((pid, _,pos), (t_uri, track_name, duration_ms, artist_uri, album_uri, album_name))) =>\n",
    "      (artist_uri, (pid, track_name, duration_ms, album_uri, album_name))\n",
    "  })\n",
    "val rddTracksInPlaylistTracksArtists = rddTracksInPlaylistTracks\n",
    "  .join(rddArtistsWithKey)\n",
    "  .map(\n",
    "    {\n",
    "      case (artist_uri, ((pid, track_name, duration_ms, album_uri, album_name), artist_name)) =>\n",
    "        (pid, artist_uri)\n",
    "    }\n",
    "  )\n",
    "\n",
    "val rddTracksInPlaylistTracksArtistsPartitioned =\n",
    "  rddTracksInPlaylistTracksArtists\n",
    "    .partitionBy(partitioner)\n",
    "    .cache()\n",
    "\n"
   ],
   "id": "14b3d2aa4ec743c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\n",
       "numPartitions: Int = 8\n",
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\n",
       "rddTrackInPlaylistsWithKey: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[10] at map at <console>:42\n",
       "rddTracksWithKey: org.apache.spark.rdd.RDD[(String, (String, Int, String, String, String))] = MapPartitionsRDD[3] at map at <console>:32\n",
       "rddArtistsWithKey: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[13] at flatMap at <console>:47\n",
       "rddTracksInPlaylistTracks: org.apache.spark.rdd.RDD[(String, (String, String, Int, String, String))] = MapPartitionsRDD[17] at map at <console>:39\n",
       "rddTracksInPlaylistTracksArtists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[21] at map at <console>:45\n",
       "rddTrack...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:43:32.626524Z",
     "start_time": "2025-01-27T14:42:37.999427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val pidArtistTrack = rddTracksInPlaylistTracksArtistsPartitioned\n",
    "  .map(x => (x, 1))\n",
    "\n",
    "// Passo 1: Calcolo del numero totale di brani per ogni artista in ogni playlist\n",
    "val artistTrackCount = pidArtistTrack.reduceByKey(_ + _) // (PID, artist_uri) -> conteggio\n",
    "\n",
    "// Passo 2: Calcolo della somma e del conteggio per ogni playlist\n",
    "val pidToArtistTracks = artistTrackCount.map(x => (x._1._1, x._2)) // PID -> conteggio\n",
    "\n",
    "val averageSongsPerArtist = pidToArtistTracks.aggregateByKey((0, 0))(\n",
    "  // Combina localmente (somma parziale e conteggio)\n",
    "  (acc, value) => (acc._1 + value, acc._2 + 1),\n",
    "  // Combina globalmente i risultati delle partizioni\n",
    "  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    ").mapValues { case (totalTracks, totalArtists) =>\n",
    "  totalTracks.toDouble / totalArtists\n",
    "}\n",
    "\n",
    "// Step 6: Calcolo della media complessiva\n",
    "val (sumOfAverages, totalPlaylists) = averageSongsPerArtist.mapPartitions(iter => {\n",
    "  var sum = 0.0\n",
    "  var count = 0L\n",
    "  iter.foreach {\n",
    "    case (_, avg) =>\n",
    "      sum += avg\n",
    "      count += 1\n",
    "  }\n",
    "  Iterator((sum, count))\n",
    "}).reduce {\n",
    "  case ((sum1, count1), (sum2, count2)) =>\n",
    "    (sum1 + sum2, count1 + count2)\n",
    "}\n",
    "\n",
    "val overallAverage = sumOfAverages / totalPlaylists\n",
    "\n",
    "// Passo 3: Calcolo della media complessiva\n",
    "//val totalPlaylists = averageSongsPerArtist.count() // Numero totale di playlist\n",
    "//val sumOfAverages = averageSongsPerArtist.map(_._2).sum() // Somma di tutte le medie\n",
    "\n",
    "//val overallAverage = sumOfAverages / totalPlaylists // Media complessiva\n"
   ],
   "id": "6148fe0c40268610",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pidArtistTrack: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[23] at map at <console>:27\n",
       "artistTrackCount: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[24] at reduceByKey at <console>:30\n",
       "pidToArtistTracks: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[25] at map at <console>:33\n",
       "averageSongsPerArtist: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[27] at mapValues at <console>:40\n",
       "sumOfAverages: Double = 429654.73025031225\n",
       "totalPlaylists: Long = 199000\n",
       "overallAverage: Double = 2.1590689962327247\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:43:33.307496Z",
     "start_time": "2025-01-27T14:43:32.776063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// empty cache\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ],
   "id": "c855c285c3283eb3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ecaa0db184efc12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
