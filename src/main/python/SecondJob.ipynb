{
 "cells": [
  {
   "cell_type": "code",
   "id": "5cb32323-09f8-48db-8067-4777f7187fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:14.598732Z",
     "start_time": "2025-01-23T16:01:14.160972Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e6f65-930e-4866-bd91-8cba39f3a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e302bb2-72ef-4058-a393-c94817570236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:17.195446Z",
     "start_time": "2025-01-23T16:01:16.568643Z"
    }
   },
   "source": [
    "val path_to_datasets = \"../../../datasets/processed\"\n",
    "\n",
    "val path_to_tracks = path_to_datasets + \"/tracks.csv\"\n",
    "val path_to_playlists = path_to_datasets + \"/playlists.csv\"\n",
    "val path_to_track_in_playlists = path_to_datasets + \"/tracks_in_playlist.csv\"\n",
    "val path_to_artists = path_to_datasets + \"/artists.csv\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../datasets/processed\r\n",
       "path_to_tracks: String = ../../../datasets/processed/tracks.csv\r\n",
       "path_to_playlists: String = ../../../datasets/processed/playlists.csv\r\n",
       "path_to_track_in_playlists: String = ../../../datasets/processed/tracks_in_playlist.csv\r\n",
       "path_to_artists: String = ../../../datasets/processed/artists.csv\r\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:20.174458Z",
     "start_time": "2025-01-23T16:01:18.660938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object CsvParser {\n",
    "\n",
    "  val noGenresListed = \"(no genres listed)\"\n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val quotes = \"\\\"\"\n",
    "\n",
    "  \n",
    "  // (PID, playlist_name, num_followers)\n",
    "  def parsePlayListLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (track_uri, track_name, duration_ms, artist_uri, album_uri, album_name)\n",
    "  def parseTrackLine(line: String): Option[(String, String, Int, String, String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt, input(3).trim, input(4).trim, input(5).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (artist_uri, artist_name)\n",
    "  def parseArtistLine(line: String): Option[(String, String)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // (PID, track_uri, pos)\n",
    "  def parseTrackInPlaylistLine(line: String): Option[(String, String, Int)] = {\n",
    "    try {\n",
    "      val input = line.split(commaRegex)\n",
    "      Some(input(0).trim, input(1).trim, input(2).trim.toInt)\n",
    "    } catch {\n",
    "      case _: Exception => None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "id": "344a2930231d75e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object CsvParser\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:23.420703Z",
     "start_time": "2025-01-23T16:01:22.386867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddTracks = sc.textFile(path_to_tracks).\n",
    "  flatMap(CsvParser.parseTrackLine)\n",
    "\n",
    "val rddPlaylists = sc.textFile(path_to_playlists).\n",
    "  flatMap(CsvParser.parsePlayListLine)\n",
    "\n",
    "val rddTrackInPlaylists = sc.textFile(path_to_track_in_playlists).\n",
    "  flatMap(CsvParser.parseTrackInPlaylistLine)\n",
    "\n",
    "val rddArtists = sc.textFile(path_to_artists).\n",
    "  flatMap(CsvParser.parseArtistLine)"
   ],
   "id": "195ad1db20c941cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTracks: org.apache.spark.rdd.RDD[(String, String, Int, String, String, String)] = MapPartitionsRDD[139] at flatMap at <console>:33\r\n",
       "rddPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[142] at flatMap at <console>:36\r\n",
       "rddTrackInPlaylists: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[145] at flatMap at <console>:39\r\n",
       "rddArtists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[148] at flatMap at <console>:42\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:25.891283Z",
     "start_time": "2025-01-23T16:01:25.328142Z"
    }
   },
   "cell_type": "code",
   "source": "val idSong = \"spotify:track:5xlWA3V2l7ZiqYF8Ag5EM8\"",
   "id": "5566507c682706cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idSong: String = spotify:track:5xlWA3V2l7ZiqYF8Ag5EM8\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:28.283060Z",
     "start_time": "2025-01-23T16:01:27.783135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// RDD of (pid, trackUri)\n",
    "val trackInPlaylistReduce = rddTrackInPlaylists.map(x => (x._1, x._2))\n",
    "\n",
    "// filter to keep only the id of playlist that contains the specific song\n",
    "val playlistForTrack = trackInPlaylistReduce.filter  { case (_, trackUri) => trackUri == idSong }.map(x => x._1)\n"
   ],
   "id": "773cca0eec25473d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackInPlaylistReduce: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[149] at map at <console>:30\r\n",
       "playlistForTrack: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[151] at map at <console>:33\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:30.547244Z",
     "start_time": "2025-01-23T16:01:30.018296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// trasform the collection in RDD to join with trackInPlaylistReduce\n",
    "val RDDPlaylistForTrack = playlistForTrack.map((_,null))\n",
    "\n",
    "// join\n",
    "val trackInSamePlaylists = trackInPlaylistReduce\n",
    "        .join(RDDPlaylistForTrack)\n",
    "        .filter(_._2._1 != idSong)\n",
    "        .map { case (pid, (trackUri, _)) => (pid, trackUri) } \n",
    "\n",
    "// result: RDD of (pid, trackUri) with only playlists that contains the specific song \n",
    "// and the relatives tracks"
   ],
   "id": "b3dfdbe802f22536",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RDDPlaylistForTrack: org.apache.spark.rdd.RDD[(String, Null)] = MapPartitionsRDD[152] at map at <console>:31\r\n",
       "trackInSamePlaylists: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[157] at map at <console>:37\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:32.860927Z",
     "start_time": "2025-01-23T16:01:32.147793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// create the pairs of ((mySong, otherSong), 1) for each playlist\n",
    "val rddTrackPairs = trackInSamePlaylists\n",
    "    .map { case (_, track) => ((idSong, track), 1)}\n",
    "    \n",
    "// rdd of form ((track1,track2), 1) use to count the occurrences"
   ],
   "id": "7fb0f9df7a6c3c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTrackPairs: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[158] at map at <console>:30\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:01:35.295486Z",
     "start_time": "2025-01-23T16:01:34.631242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// reduce by key to count the occurrences\n",
    "val occurrencesCount = rddTrackPairs.reduceByKey(_ + _)\n"
   ],
   "id": "cdddc6bd9aabdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occurrencesCount: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[159] at reduceByKey at <console>:28\r\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:02:12.136049Z",
     "start_time": "2025-01-23T16:01:36.700897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// take the pair with the highest count\n",
    "val mostOccurrencesPair = occurrencesCount\n",
    "          .reduce((x, y) => if (x._2 > y._2) x else y)\n"
   ],
   "id": "5c48afeae3ad6d65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostOccurrencesPair: ((String, String), Int) = ((spotify:track:5xlWA3V2l7ZiqYF8Ag5EM8,spotify:track:4agZCOTdZDD4r33mPPDy8b),2)\r\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T16:02:39.612559Z",
     "start_time": "2025-01-23T16:02:21.237939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// Unisci i dettagli della traccia specifica e delle tracce correlate\n",
    "val trackDetails = rddTracks.map(x => (x._1, x._2)) // (trackUri, trackName)\n",
    "\n",
    "// mostOccurrencesPair to rdd to join \n",
    "val mostOccurrencesPairRDD = sc.parallelize(Seq(mostOccurrencesPair))\n",
    "\n",
    "val enrichedResults = mostOccurrencesPairRDD\n",
    "        .map {case ((track1, track2), count) => (track1, (track2, count)) }\n",
    "        .join(trackDetails) // Unisci il nome della traccia principale\n",
    "        .map { case (track1, ((track2, count), track1Name)) => (track2, (track1, track1Name, count)) }\n",
    "        .join(trackDetails) // Unisci il nome delle co-tracce\n",
    "        .map { case (track2, ((track1, track1Name, count), track2Name)) => (track1, track1Name, track2, track2Name, count) }\n",
    "\n",
    "// Salva il risultato\n",
    "enrichedResults.coalesce(1).saveAsTextFile(\"output/result\")"
   ],
   "id": "5d8e94020446b6e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackDetails: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[160] at map at <console>:33\r\n",
       "mostOccurrencesPairRDD: org.apache.spark.rdd.RDD[((String, String), Int)] = ParallelCollectionRDD[161] at parallelize at <console>:36\r\n",
       "enrichedResults: org.apache.spark.rdd.RDD[(String, String, String, String, Int)] = MapPartitionsRDD[170] at map at <console>:43\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T16:03:35.153426Z",
     "start_time": "2025-01-22T16:03:35.057613Z"
    }
   },
   "cell_type": "code",
   "source": [
    " /*\n",
    "\n",
    "       // PER TUTTE LE CANZONI, DA FARE CON POCHI FILE\n",
    "\n",
    "// For every song, find the most similar song, i.e., the one that appears most frequently in the same playlist.\n",
    "// (Join-Join-Aggregate)\n",
    "\n",
    "// auto-join of track_in_playlist \n",
    "// to get all song pairs in the same playlist\n",
    "\n",
    "// RDD of (pid, trackUri)\n",
    "val playlistTracks = rddTrackInPlaylists.map(x => (x._1, x._2))\n",
    "\n",
    "val rddTrackPairs = playlistTracks.join(playlistTracks)\n",
    "    .filter { case (_, (track1, track2)) => track1 != track2 }  // no auto-pair\n",
    "    .map { case (_, (track1, track2)) => ((track1, track2), 1) }   \n",
    "\n",
    "//Forma: ((track1, track2), 1)"
   ],
   "id": "8e4fa51c048f407f",
   "outputs": [
    {
     "ename": "<console>",
     "evalue": " error: incomplete input",
     "output_type": "error",
     "traceback": [
      "<console>: error: incomplete input"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:34:15.544834Z",
     "start_time": "2025-01-21T16:34:14.523193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// count the number of times each pair appears together\n",
    "val cooccurencesCount = rddTrackPairs.reduceByKey(_ + _)"
   ],
   "id": "e61bd3780e1b0a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cooccurencesCount: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[56] at reduceByKey at <console>:26\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:34:18.841995Z",
     "start_time": "2025-01-21T16:34:17.571022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// get the most common pair for each track\n",
    "val mostCommonPair = cooccurencesCount\n",
    "          .map { case ((track1, track2), count) => (track1, (track2, count)) }\n",
    "          .reduceByKey { case ((track2A, count1), (track2B, count2)) => if (count1 > count2) (track2A, count1) else (track2B, count2) }"
   ],
   "id": "7ad8fe5a195ce674",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mostCommonPair: org.apache.spark.rdd.RDD[(String, (String, Int))] = ShuffledRDD[58] at reduceByKey at <console>:28\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:48:58.865791Z",
     "start_time": "2025-01-21T01:43:14.873705Z"
    }
   },
   "cell_type": "code",
   "source": "//mostCommonPair.collect()",
   "id": "cc8dd895138ba16e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, (String, Int))] = Array((spotify:track:1mjcyWQPFsoG1Cb6Gl33Tk,(spotify:track:3kAwGo8mtUPsrctroxYLku,2)), (spotify:track:7y7bmax41rKlTLOmgNvzKR,(spotify:track:4awpwf3TeFWOtLiswRbKfr,1)), (spotify:track:7inXu0Eaeg02VsM8kHNvzM,(spotify:track:3T7dNA7O8c3Axj5WyDNcH3,11)), (spotify:track:1WHCuLyhiISWVENR0qXZ51,(spotify:track:57TUYBa41jfW56U2U9652l,1)), (spotify:track:7ccnwVhaD3ITUQ2x8EkilA,(spotify:track:5xoUgPXbMNUmoHU0Enwtwq,2)), (spotify:track:5oUV6yWdDM0R9Q2CizRhIt,(spotify:track:3ZMv9EzGoteNi5Qnx0KpEO,5)), (spotify:track:7AUOZzM5P8UDuA0zga0PP8,(spotify:track:57yfmPoMfWljcEl3qI1ADp,2)), (spotify:track:4bGEfWw5uEAnvYuTbESsMa,(spotify:track:4CbKVDZkYKdv69I4bCaKUq,1)), (spotify:track:2m6Wm0nBUJdDfCggvpWAnV,(spotify:track:12TE7Vt592RcM1G3EaaZ0f,2)), (spotify:track:3CpoeW0...\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:34:25.343655Z",
     "start_time": "2025-01-21T16:34:23.479877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// RDD of (track_uri, name)\n",
    "val trackDetails = rddTracks.map(x => (x._1, x._2))\n",
    "\n",
    "// join with track details\n",
    "val firstResult = mostCommonPair.\n",
    "        join(trackDetails).\n",
    "        map { case (track1, ((track2, count), track1Name)) => (track1, track1Name, track2, count) }\n",
    "\n"
   ],
   "id": "9a47acd97ce4ff84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trackDetails: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[59] at map at <console>:27\r\n",
       "firstResult: org.apache.spark.rdd.RDD[(String, String, String, Int)] = MapPartitionsRDD[63] at map at <console>:32\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:34:36.714262Z",
     "start_time": "2025-01-21T16:34:35.788219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val finalResult = firstResult\n",
    "        .map { case (track1, track1Name, track2, count) => (track2, (track1, track1Name, count)) }\n",
    "        .join(trackDetails)\n",
    "        .map { case (track2, ((track1, track1Name, count), track2Name)) =>\n",
    "          (track1, track1Name, track2, track2Name, count)\n",
    "        }\n",
    "        "
   ],
   "id": "e7899f0c95c0a6b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalResult: org.apache.spark.rdd.RDD[(String, String, String, String, Int)] = MapPartitionsRDD[68] at map at <console>:29\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:41:29.487535Z",
     "start_time": "2025-01-21T16:35:11.828813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val csv = finalResult.map {\n",
    "  case (trackUri, trackName, coTrackUri, coTrackName, count) =>\n",
    "    s\"$trackUri,$trackName,$coTrackUri,$coTrackName,$count\"\n",
    "}\n",
    "\n",
    "csv.saveAsTextFile(\"output/result\")\n",
    "\n",
    " */"
   ],
   "id": "ab1022c59c1c5733",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[69] at map at <console>:25\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
